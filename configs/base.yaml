# configs/base.yaml: Common default configurations (RECONSTRUCTED)

# Schema version to ensure config compatibility
schema_version: "1.0"

project:
  name: "asys-i-default"
  output_dir: "./outputs/default"
  log_dir: "./logs/default"
  checkpoint_dir: "./checkpoints/default"
  log_level: "INFO"
  seed: 42

hardware:
  device: "cuda:0"
  dtype: "bfloat16" # More robust default for modern GPUs
  use_mixed_precision: true
  compile_model: true

ppo:
   model_name: "gpt2"
   max_steps: 1000

sae_model:
   d_in: "auto" # Auto-detect from host model's hidden_size
   d_sae: 3072 # 4x expansion for d_in=768
   l1_coefficient: 0.004

sae_trainer:
   learning_rate: 0.0004
   batch_size: 4096
   num_workers: 4
   save_interval_steps: 10000
   normalize_weights_interval: 100 # New: Perform weight normalization periodically

hook:
   # New FQN format for hooking
   layers_to_hook:
     "mlp_3": "transformer.h.3.mlp.c_proj"
     "attn_6": "transformer.h.6.attn.c_proj"
     "mlp_9": "transformer.h.9.mlp.c_proj"
   sampling_rate: 1.0
   gpu_kernel_mode: "NONE" # FP8, LZ4 etc. are future work
   pinned_memory_size_bytes: 0 # If 0 or not set, an estimated size will be used (e.g., 64MB or based on max_elements_estimate).
                               # Example: pinned_memory_size_bytes: 67108864 # 64MB

# Defaults set by config_loader based on run_profile if not specified
# data_bus:
# monitor:

archiver:
   enabled: false # Disabled by default for performance
   batch_size: 8192

resource_manager:
    apply_bindings: false
    allocation_strategy: "auto"
